%% bare_conf_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


\documentclass[conference,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{url}
\usepackage{placeins}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{graphicx}
\graphicspath{{results/}{results/rlchef/}}

\begin{document}

\title{RL Chef: Tabular Reinforcement Learning vs Linear Function Approximation \\ and the Impact of State Representation}

\author{\IEEEauthorblockN{Luigi Daddario (mat. 908294)}
\IEEEauthorblockA{Artificial Intelligence For Science And Technology\\
University of Milano-Bicocca\\
Email: l.daddario1@campus.unimib.it}}

\maketitle

\begin{abstract}
We study a custom Gymnasium environment, \emph{RL Chef}, where an agent navigates a $5\times5$ grid, collects ingredients under supply constraints, and decides when to cook a dish to maximize reward. We compare tabular control (Q-learning) with linear function approximation for the action-value function, and we analyze how state representation affects learning and convergence. In particular, we contrast an intentionally lossy, non-Markov representation (\texttt{simple}) with a Markov representation that augments the state with a visited-cells mask (\texttt{mask}). Across multiple random seeds we report learning curves, final evaluation return, and interpretable domain metrics (waste and incompatible ingredient usage).
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Reinforcement learning (RL) studies how an agent can learn to act optimally through interaction with an environment modeled as a Markov Decision Process (MDP) \cite{SuttonBarto2018}. In small discrete domains, tabular methods such as Q-learning can converge to the optimal action-value function under standard assumptions \cite{SuttonBarto2018}. In larger state spaces, value function approximation is typically required to enable generalization \cite{SuttonBarto2018}.

This report focuses on \emph{RL Chef}, a compact gridworld designed to highlight two key themes: (i) the trade-off between tabular control and linear function approximation, and (ii) the critical role of state representation and Markovity. The environment includes delayed decision-making (choosing when to cook), penalties for waste, and constraints induced by one-time ingredient availability, creating a non-trivial exploration--exploitation problem even on a small grid.

\section{Objectives}
The objectives of this project are:
\begin{itemize}
\item Model the cooking task as an MDP and implement it with the Gymnasium API \cite{Gymnasium}.
\item Implement and compare:
  \begin{itemize}
  \item tabular Q-learning (and optionally SARSA);
  \item linear action-value approximation trained with temporal-difference updates.
  \end{itemize}
\item Analyze the impact of state representation by comparing an intentionally lossy, non-Markov representation (\texttt{simple}) with a Markov representation that augments the state with a visited-cells mask (\texttt{mask}).
\item Produce reproducible results across multiple seeds, including learning curves, evaluation metrics, and domain-specific diagnostics.
\end{itemize}

\section{Methods}

\subsection{Dataset}
No external dataset is used. All experience is generated online by interacting with the environment and collecting trajectories of $(s_t, a_t, r_t, s_{t+1})$.

\subsection{Optimization and Regularization}
We use $\epsilon$-greedy exploration with an optional linear decay schedule $\epsilon_t$ from $\epsilon_0$ to $\epsilon_{\mathrm{final}}$ over a fixed number of episodes. The discount factor is fixed to $\gamma \in (0,1)$. For tabular control we update a Q-table (Q-learning update); for linear approximation we parameterize:
\[
Q_\theta(s,a) = w_a^\top \phi(s) + b_a
\]
and perform semi-gradient temporal-difference updates on the parameters. L2 regularization is available in the implementation but is set to $0$ in the main experiments.

\subsection{Environment and State Representations}
The environment is a $5\times5$ grid. Each cell contains one ingredient type; ingredients can be collected at most once per episode (supply constraint). The agent chooses among 5 actions: 4-direction movement plus \texttt{cook}. The \texttt{cook} action ends the episode and yields a reward based on the best feasible recipe given the inventory, minus penalties for waste and incompatible ingredient combinations.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{rlchef/env_layout.png}
\caption{Static overview of the fixed grid layout used in experiments (ingredient id and short name per cell).}
\label{fig:env_layout}
\end{figure}

\begin{table}[t]
\centering
\caption{Environment parameters and recipe definitions (auto-generated from code).}
\label{tab:env_specs}
\input{results/rlchef/env_specs.tex}
\end{table}

We compare two discrete state keys for tabular control:
\begin{itemize}
\item \textbf{\texttt{simple}}: position and inventory counts (lossy / non-Markov because it omits which cells have been depleted).
\item \textbf{\texttt{mask}}: augments \texttt{simple} with a bitmask of visited/depleted cells, restoring Markovity under a fixed grid layout.
\end{itemize}

Both agents also receive a continuous observation vector with normalized features (position, current-cell ingredient one-hot, availability, inventory, remaining supply; and optional budget/round features in the challenging variant).

\section{Experiments}
We run a benchmark that compares four configurations across multiple random seeds:
\begin{itemize}
\item Q-learning with \texttt{simple} state;
\item Q-learning with \texttt{mask} state;
\item linear function approximation with \texttt{simple} observation;
\item linear function approximation with \texttt{mask} observation.
\end{itemize}

The main results in this paper use the \textbf{base} environment variant (no budget constraint; single-round episodes). We additionally report results for a \textbf{budget} variant, where the agent must pay ingredient costs from an initial budget and receives non-negative revenue when cooking (default settings: $1$ round, start budget $=10$, no debt).

Each run trains for 20{,}000 episodes with $\epsilon$-greedy exploration ($\epsilon_0=0.3 \rightarrow \epsilon_{\mathrm{final}}=0.05$ linearly over 3{,}000 episodes) and uses 10 random seeds. Final evaluation sets $\epsilon=0$ and estimates metrics over 1{,}000 evaluation episodes per run. We report the mean and standard deviation (across seeds) of evaluation return, average steps, and domain-specific diagnostics (waste and incompatibility). Additionally, we compute sample-efficiency proxies from the training curve: the average training return over all episodes (AUC proxy) and the mean return over the last episodes (stability / late performance).

\subsection{Implementation details and reproducibility}
The base benchmark can be reproduced by running \texttt{python -m rlchef.experiments --variant base} followed by \texttt{python -m rlchef.analyze} (outputs in \texttt{results/rlchef/}). The budget benchmark can be reproduced similarly with \texttt{--variant budget} (outputs in \texttt{results/rlchef\_budget/}).

Compute cost is modest for this environment: on a standard macOS laptop, a single seed run with 20{,}000 training episodes (covering all four configurations) takes on the order of tens of seconds, and the 10-seed benchmark completes within minutes; plotting/aggregation adds negligible overhead relative to training.\footnote{Wall-clock time depends on hardware and Python environment; the goal here is to provide an order-of-magnitude estimate.}

\subsection{Results}
Table~\ref{tab:metrics} reports aggregate evaluation metrics across seeds. Q-learning achieves high return with low variability: $3.307\pm0.104$ for \texttt{simple} and $3.177\pm0.079$ for \texttt{mask}. In contrast, the linear baseline is considerably less consistent, achieving $1.652\pm0.718$ in evaluation (Table~\ref{tab:metrics}) and showing a wide inter-seed dispersion (Figure~\ref{fig:boxplot}).

Figure~\ref{fig:aggregate} summarizes training dynamics (moving average; mean $\pm$ std across seeds). Tabular learning reaches a stable plateau within a few thousand episodes. The \texttt{mask} representation does not improve over \texttt{simple} within our training budget; a plausible explanation is that augmenting the state with a visited-cells mask increases the effective tabular state space, which can slow down learning despite restoring Markovity in principle. Under a fixed grid layout, the lossy \texttt{simple} key may still be sufficient to learn a near-optimal policy for this task.

Beyond scalar return, domain diagnostics reveal qualitative policy differences. Q-learning yields short episodes ($\approx 4$ steps on average), suggesting it learns to collect a small, high-value set of ingredients and cook quickly (Figure~\ref{fig:steps}). The linear baseline produces significantly longer episodes ($25.02\pm13.26$ steps), higher waste ($2.18\pm1.41$), and a broader trade-off curve (Figure~\ref{fig:tradeoff}), consistent with slower/less decisive policies and suboptimal ingredient collection. Figure~\ref{fig:made} shows that the tabular policy produces a more diverse recipe mix, while the linear baseline concentrates on a smaller set of outcomes and exhibits a larger fraction of failures (no feasible recipe cooked).

Finally, Figure~\ref{fig:sample_eff} compares sample-efficiency proxies derived from training curves. Q-learning achieves higher average train return (AUC proxy) and higher late-stage performance, while the linear baseline is lower on both metrics, matching the observed evaluation gap.

\subsection{Budget-variant results (additional experiment)}
Table~\ref{tab:metrics_budget} and Figure~\ref{fig:aggregate_budget} report the same benchmark under the budget constraint. Under these default budget settings, the task becomes significantly harder: tabular policies achieve low but stable positive returns ($0.427\pm0.008$ for \texttt{simple}, $0.360\pm0.030$ for \texttt{mask}), while the linear baseline collapses to a constant failure outcome ($-1.200\pm0.000$).

\subsection{Discussion and Limitations}
These results highlight a key practical point: in small, structured discrete environments, tabular control can be both data-efficient and stable \cite{SuttonBarto2018,Mannor2023}. The linear baseline here uses a simple feature vector and an off-policy TD update, which can be sensitive to feature design, exploration schedule, and reward shaping. High variance across seeds (Figure~\ref{fig:boxplot}) suggests that learning outcomes depend strongly on early trajectories and exploration decisions.

The \texttt{simple}/\texttt{mask} comparison should be interpreted carefully for function approximation. In our implementation, \texttt{state\_mode} affects only the discrete tabular key; the observation features used by the linear model are unchanged, hence \texttt{simple} and \texttt{mask} are expected to be identical for the linear baseline (as observed in Table~\ref{tab:metrics} and Figures~\ref{fig:steps}--\ref{fig:sample_eff}).

Limitations include the absence of extensive hyperparameter tuning and the simplicity of the linear approximator. Future work could (i) add the visited-cells mask (or a learned memory) to the feature representation for function approximation, (ii) evaluate on randomized grid layouts for robustness, and (iii) compare additional baselines (e.g., SARSA, eligibility traces, or more stable variants/analyses for temporal-difference learning with function approximation \cite{Patil2023}).

\section{Conclusion}
We presented \emph{RL Chef}, a compact Gymnasium environment designed to compare tabular control with linear value function approximation and to study the impact of state representation. Across 10 seeds, tabular Q-learning reliably achieves high return with low variability, while the linear baseline is less stable and incurs higher waste and longer episodes. The analysis illustrates how representational choices and approximation can dominate performance even in small environments, motivating careful state design and more expressive function approximation for scalable RL.

\section{Visualizations}
Figure~\ref{fig:aggregate} shows aggregated learning curves (moving average window $=200$ episodes; mean $\pm$ std across seeds). Table~\ref{tab:metrics} summarizes the aggregate evaluation metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{rlchef/returns_aggregate.png}
\caption{Aggregated learning curves (moving average; train return) across seeds: mean $\pm$ std.}
\label{fig:aggregate}
\end{figure}

\begin{table*}[t]
\centering
\caption{Aggregate evaluation metrics across seeds (mean $\pm$ std).}
\label{tab:metrics}
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\input{results/rlchef/metrics_table.tex}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{rlchef_budget/returns_aggregate.png}
\caption{Budget variant: aggregated learning curves (moving average; train return) across seeds: mean $\pm$ std.}
\label{fig:aggregate_budget}
\end{figure}

\begin{table*}[t]
\centering
\caption{Budget variant: aggregate evaluation metrics across seeds (mean $\pm$ std).}
\label{tab:metrics_budget}
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\input{results/rlchef_budget/metrics_table.tex}
\end{table*}

\begin{figure*}[t]
\centering
\begin{minipage}[t]{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{rlchef/eval_return_boxplot.png}
\caption{Evaluation return distribution across seeds (boxplot).}
\label{fig:boxplot}
\end{minipage}\hfill
\begin{minipage}[t]{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{rlchef/steps_bars.png}
\caption{Episode length in evaluation (mean $\pm$ std across seeds).}
\label{fig:steps}
\end{minipage}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{minipage}[t]{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{rlchef/waste_incompat_bars.png}
\caption{Domain diagnostics: waste and incompatibility (mean $\pm$ std across seeds).}
\label{fig:waste_incompat}
\end{minipage}\hfill
\begin{minipage}[t]{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{rlchef/tradeoff_scatter.png}
\caption{Run-level trade-off: evaluation return vs penalties (waste + incompatibility).}
\label{fig:tradeoff}
\end{minipage}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{minipage}[t]{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{rlchef/made_distribution.png}
\caption{Which dishes are produced? Stacked proportions over evaluation episodes; \texttt{fail} indicates no feasible recipe cooked.}
\label{fig:made}
\end{minipage}\hfill
\begin{minipage}[t]{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{rlchef/sample_efficiency.png}
\caption{Training-curve proxies: mean train return (AUC proxy) and last-episodes mean (stability), mean $\pm$ std across seeds.}
\label{fig:sample_eff}
\end{minipage}
\end{figure*}

\FloatBarrier

\ifCLASSOPTIONcompsoc
  \section*{Disclosure Statement}
\else
  \section*{Disclosure Statement}
\fi

The author declares that this report is entirely original and does not contain any plagiarism.

\begin{thebibliography}{1}

\bibitem{Mannor2023}
S. Mannor, Y. Mansour, and A. Tamar, \emph{Reinforcement Learning: Foundations}, 2023 (last update Nov. 2025). [Online]. Available: \url{https://sites.google.com/view/rlfoundations/home}

\bibitem{SuttonBarto2018}
R. S. Sutton and A. G. Barto, \emph{Reinforcement Learning: An Introduction}, 2nd ed. MIT Press, 2018. [Online]. Available: \url{http://incompleteideas.net/book/the-book-2nd.html}

\bibitem{Gymnasium}
Farama Foundation, ``Gymnasium: A Standard API for Reinforcement Learning Environments,'' Accessed: Jan. 20, 2026. [Online]. Available: \url{https://gymnasium.farama.org/}

\bibitem{Patil2023}
G. Patil et al., ``Finite time analysis of temporal difference learning with linear function approximation,'' in \emph{Proc. Intl. Conf. Mach. Learn. (ICML)}, Jul. 2023. [Online]. Available: \url{https://proceedings.mlr.press/v206/patil23a.html}

\end{thebibliography}

\end{document}
